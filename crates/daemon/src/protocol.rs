//! DataCraft Protocol Handler
//!
//! Integrates DataCraft with libp2p swarm, handles DHT operations and incoming transfer streams.

use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;

use datacraft_core::{ContentId, ContentManifest, TRANSFER_PROTOCOL};
use datacraft_routing::ContentRouter;
use datacraft_store::FsStore;
use datacraft_transfer::{
    decode_piece_request, decode_manifest_push_header,
    encode_piece_response, encode_piece_response_error,
    PIECE_REQUEST_SIZE, MANIFEST_PUSH_HEADER_SIZE,
};
use craftec_network::{CraftBehaviour, behaviour::CraftBehaviourEvent};
use libp2p::kad;
use libp2p::swarm::SwarmEvent;
use libp2p_stream::IncomingStreams;
use sha2::{Sha256, Digest};
use tokio::sync::{mpsc, Mutex};
use tracing::{debug, error, info, warn};

use crate::errors::TransferError;
use crate::events::{DaemonEvent, EventSender};
use crate::receipt_store::PersistentReceiptStore;
use crate::stream_manager::StreamPool;

/// Default timeout for read operations (30 seconds).
const DEFAULT_READ_TIMEOUT_SECS: u64 = 30;
/// Timeout for write operations (15 seconds).
const WRITE_TIMEOUT: Duration = Duration::from_secs(15);
/// Default timeout for opening a new stream (10 seconds).
const DEFAULT_STREAM_OPEN_TIMEOUT_SECS: u64 = 10;
/// Fallback stream open timeout used by static methods that don't have access to config.
const STREAM_OPEN_TIMEOUT: Duration = Duration::from_secs(DEFAULT_STREAM_OPEN_TIMEOUT_SECS);
/// Maximum retry attempts for transient errors.
const MAX_RETRIES: u32 = 3;
/// Minimum disk space threshold in bytes (100 MB).
const DISK_SPACE_THRESHOLD: u64 = 100 * 1024 * 1024;
/// Maximum piece data size (16 MB).
const MAX_PIECE_DATA_SIZE: usize = 16 * 1024 * 1024;
/// Maximum coefficient vector size (64 KB — k=100 segments is typical, with some headroom).
const MAX_COEFF_SIZE: usize = 64 * 1024;
/// Maximum manifest payload size (10 MB).
const MAX_MANIFEST_SIZE: usize = 10 * 1024 * 1024;
/// Maximum inventory payload size (50 MB).
const MAX_INVENTORY_SIZE: usize = 50 * 1024 * 1024;

/// Events generated by the DataCraft protocol handler.
#[derive(Debug)]
pub enum DataCraftEvent {
    /// DHT provider query completed.
    ProvidersResolved {
        content_id: ContentId,
        providers: Vec<libp2p::PeerId>,
    },
    /// DHT manifest record retrieved.
    ManifestRetrieved {
        content_id: ContentId,
        manifest: ContentManifest,
    },
    /// DHT access list record retrieved.
    AccessListRetrieved {
        content_id: ContentId,
        access_list: datacraft_core::access::AccessList,
    },
    /// DHT operation failed.
    DhtError {
        content_id: ContentId,
        error: String,
    },
    /// Manifest received via push from another node (we're storing for them).
    ManifestPushReceived {
        content_id: ContentId,
        manifest: ContentManifest,
    },
    /// Piece received via push — update local piece count in tracker.
    PiecePushReceived {
        content_id: ContentId,
    },
}

/// DataCraft protocol handler for registration on shared libp2p swarm.
pub struct DataCraftProtocol {
    /// Local content store.
    store: Arc<Mutex<FsStore>>,
    /// Event sender to the daemon.
    event_tx: mpsc::UnboundedSender<DataCraftEvent>,
    /// Pending DHT queries waiting for completion.
    pending_queries: Arc<Mutex<HashMap<kad::QueryId, PendingQuery>>>,
    /// Persistent receipt store for durable receipt storage.
    persistent_receipt_store: Option<Arc<Mutex<PersistentReceiptStore>>>,
    /// Shared removal cache — checked before serving pieces.
    removal_cache: Option<Arc<Mutex<crate::removal_cache::RemovalCache>>>,
    /// Demand tracker for scaling — records fetches when serving pieces.
    demand_tracker: Option<Arc<Mutex<crate::scaling::DemandTracker>>>,
    /// Eviction manager — records access for LRU tracking.
    eviction_manager: Option<Arc<Mutex<crate::eviction::EvictionManager>>>,
    /// Storage Merkle tree — updated on piece store/delete.
    merkle_tree: Option<Arc<Mutex<datacraft_store::merkle::StorageMerkleTree>>>,
    /// Stream pool for persistent outbound streams per peer.
    stream_pool: Option<Arc<Mutex<StreamPool>>>,
    /// Bounded concurrency semaphore for inbound piece processing.
    inbound_semaphore: Arc<tokio::sync::Semaphore>,
    /// Daemon event sender for UI error propagation.
    daemon_event_tx: Option<EventSender>,
    /// Peer scorer for tracking bad data.
    peer_scorer: Option<Arc<Mutex<crate::peer_scorer::PeerScorer>>>,
    /// Configurable piece timeout (read timeout) in seconds.
    piece_timeout_secs: u64,
    /// Configurable stream open timeout in seconds.
    stream_open_timeout_secs: u64,
}

/// Tracks what we're waiting for from a DHT query.
#[derive(Debug)]
enum PendingQuery {
    ProvidersLookup { content_id: ContentId },
    ManifestLookup { content_id: ContentId },
    AccessListLookup { content_id: ContentId },
}

/// Helper: wrap a read operation with timeout.
async fn timed_read<F, T>(fut: F) -> Result<T, TransferError>
where
    F: std::future::Future<Output = Result<T, std::io::Error>>,
{
    timed_read_secs(fut, DEFAULT_READ_TIMEOUT_SECS).await
}

/// Helper: wrap a read operation with configurable timeout.
async fn timed_read_secs<F, T>(fut: F, timeout_secs: u64) -> Result<T, TransferError>
where
    F: std::future::Future<Output = Result<T, std::io::Error>>,
{
    match tokio::time::timeout(Duration::from_secs(timeout_secs), fut).await {
        Ok(Ok(val)) => Ok(val),
        Ok(Err(e)) => Err(TransferError::from_io(&e)),
        Err(_) => Err(TransferError::Timeout),
    }
}

/// Helper: wrap a write operation with timeout.
async fn timed_write<F, T>(fut: F) -> Result<T, TransferError>
where
    F: std::future::Future<Output = Result<T, std::io::Error>>,
{
    match tokio::time::timeout(WRITE_TIMEOUT, fut).await {
        Ok(Ok(val)) => Ok(val),
        Ok(Err(e)) => Err(TransferError::from_io(&e)),
        Err(_) => Err(TransferError::Timeout),
    }
}

/// Write data and flush in one call. Without flush, data may sit in yamux buffers
/// and never reach the remote peer (causing both sides to hang).
async fn write_and_flush(stream: &mut libp2p::Stream, data: &[u8]) -> Result<(), TransferError> {
    use futures::AsyncWriteExt;
    timed_write(stream.write_all(data)).await?;
    timed_write(stream.flush()).await?;
    Ok(())
}

/// Check available disk space at the given path. Returns bytes available.
fn available_disk_space(path: &std::path::Path) -> Option<u64> {
    #[cfg(unix)]
    {
        use std::os::unix::ffi::OsStrExt;
        let c_path = std::ffi::CString::new(path.as_os_str().as_bytes()).ok()?;
        let mut stat: libc::statvfs = unsafe { std::mem::zeroed() };
        let ret = unsafe { libc::statvfs(c_path.as_ptr(), &mut stat) };
        if ret == 0 {
            Some(stat.f_bavail as u64 * stat.f_frsize as u64)
        } else {
            None
        }
    }
    #[cfg(not(unix))]
    {
        let _ = path;
        None // Conservative: skip check on non-unix
    }
}

/// Validate that piece_id == SHA-256(coefficients).
fn validate_piece_id(piece_id: &[u8; 32], coefficients: &[u8]) -> bool {
    let mut hasher = Sha256::new();
    hasher.update(coefficients);
    let computed: [u8; 32] = hasher.finalize().into();
    computed == *piece_id
}

impl DataCraftProtocol {
    /// Create a new DataCraft protocol handler.
    pub fn new(
        store: Arc<Mutex<FsStore>>,
        event_tx: mpsc::UnboundedSender<DataCraftEvent>,
    ) -> Self {
        Self {
            store,
            event_tx,
            pending_queries: Arc::new(Mutex::new(HashMap::new())),
            persistent_receipt_store: None,
            removal_cache: None,
            demand_tracker: None,
            eviction_manager: None,
            merkle_tree: None,
            stream_pool: None,
            inbound_semaphore: Arc::new(tokio::sync::Semaphore::new(64)),
            daemon_event_tx: None,
            peer_scorer: None,
            piece_timeout_secs: DEFAULT_READ_TIMEOUT_SECS,
            stream_open_timeout_secs: DEFAULT_STREAM_OPEN_TIMEOUT_SECS,
        }
    }

    /// Set the persistent receipt store for durable storage.
    pub fn set_persistent_receipt_store(&mut self, store: Arc<Mutex<PersistentReceiptStore>>) {
        self.persistent_receipt_store = Some(store);
    }

    /// Set the removal cache for pre-serve checks.
    pub fn set_removal_cache(&mut self, cache: Arc<Mutex<crate::removal_cache::RemovalCache>>) {
        self.removal_cache = Some(cache);
    }

    pub fn set_demand_tracker(&mut self, tracker: Arc<Mutex<crate::scaling::DemandTracker>>) {
        self.demand_tracker = Some(tracker);
    }

    pub fn set_eviction_manager(&mut self, mgr: Arc<Mutex<crate::eviction::EvictionManager>>) {
        self.eviction_manager = Some(mgr);
    }

    pub fn set_merkle_tree(&mut self, tree: Arc<Mutex<datacraft_store::merkle::StorageMerkleTree>>) {
        self.merkle_tree = Some(tree);
    }

    pub fn set_stream_pool(&mut self, pool: Arc<Mutex<StreamPool>>) {
        self.stream_pool = Some(pool);
    }

    pub fn set_daemon_event_tx(&mut self, tx: EventSender) {
        self.daemon_event_tx = Some(tx);
    }

    pub fn set_peer_scorer(&mut self, scorer: Arc<Mutex<crate::peer_scorer::PeerScorer>>) {
        self.peer_scorer = Some(scorer);
    }

    /// Configure protocol limits from daemon config.
    pub fn set_limits(&mut self, max_concurrent_transfers: usize, piece_timeout_secs: u64, stream_open_timeout_secs: u64) {
        self.inbound_semaphore = Arc::new(tokio::sync::Semaphore::new(max_concurrent_transfers));
        self.piece_timeout_secs = piece_timeout_secs;
        self.stream_open_timeout_secs = stream_open_timeout_secs;
    }

    /// Emit a transfer error event to the UI.
    fn emit_transfer_error(&self, content_id: &ContentId, peer_id: &libp2p::PeerId, error: &TransferError) {
        if let Some(ref tx) = self.daemon_event_tx {
            let _ = tx.send(DaemonEvent::TransferError {
                content_id: content_id.to_hex(),
                peer_id: peer_id.to_string(),
                error_type: error.error_type().to_string(),
                message: error.to_string(),
            });
        }
    }

    /// Emit a storage pressure event to the UI.
    fn emit_storage_pressure(&self, available_bytes: u64) {
        if let Some(ref tx) = self.daemon_event_tx {
            let _ = tx.send(DaemonEvent::StoragePressure {
                available_bytes,
                threshold_bytes: DISK_SPACE_THRESHOLD,
            });
        }
    }

    /// Increment bad_data counter for a peer in the scorer.
    fn record_bad_data(&self, peer_id: &libp2p::PeerId) {
        if let Some(ref scorer) = self.peer_scorer {
            if let Ok(mut s) = scorer.try_lock() {
                s.record_failure(peer_id);
                // Double-count: failures are weighted, but we record twice for bad data
                s.record_failure(peer_id);
            }
        }
    }

    /// Register this protocol with the shared libp2p swarm.
    /// Returns incoming transfer streams.
    pub fn register(
        &self,
        swarm: &mut craftec_network::CraftSwarm,
    ) -> Result<IncomingStreams, Box<dyn std::error::Error>> {
        debug!("Registering DataCraft transfer protocol: {}", TRANSFER_PROTOCOL);

        let mut control = swarm.behaviour().stream_control();
        let transfer_streams = control.accept(libp2p::StreamProtocol::new(TRANSFER_PROTOCOL))?;

        Ok(transfer_streams)
    }

    /// Announce this node as a provider for the given content ID.
    pub async fn announce_provider(
        &self,
        behaviour: &mut CraftBehaviour,
        content_id: &ContentId,
    ) -> Result<(), Box<dyn std::error::Error>> {
        debug!("Announcing as provider for {}", content_id);
        ContentRouter::announce(behaviour, content_id)?;
        Ok(())
    }

    /// Publish a manifest to the DHT.
    pub async fn publish_manifest(
        &self,
        behaviour: &mut CraftBehaviour,
        manifest: &ContentManifest,
        local_peer_id: &libp2p::PeerId,
    ) -> Result<(), Box<dyn std::error::Error>> {
        debug!("Publishing manifest for {} to DHT", manifest.content_id);
        ContentRouter::publish_manifest(behaviour, manifest, local_peer_id)?;
        Ok(())
    }

    /// Resolve providers for a content ID via DHT.
    pub async fn resolve_providers(
        &self,
        behaviour: &mut CraftBehaviour,
        content_id: &ContentId,
    ) -> Result<(), Box<dyn std::error::Error>> {
        debug!("Resolving providers for {}", content_id);
        let query_id = ContentRouter::resolve(behaviour, content_id);

        let mut queries = self.pending_queries.lock().await;
        queries.insert(query_id, PendingQuery::ProvidersLookup {
            content_id: *content_id
        });

        Ok(())
    }

    /// Get a manifest from the DHT.
    pub async fn get_manifest(
        &self,
        behaviour: &mut CraftBehaviour,
        content_id: &ContentId,
    ) -> Result<(), Box<dyn std::error::Error>> {
        debug!("Getting manifest for {}", content_id);
        let query_id = ContentRouter::get_manifest(behaviour, content_id);

        let mut queries = self.pending_queries.lock().await;
        queries.insert(query_id, PendingQuery::ManifestLookup {
            content_id: *content_id
        });

        Ok(())
    }

    /// Get an access list from the DHT (tracked for async response).
    pub async fn get_access_list_dht(
        &self,
        behaviour: &mut CraftBehaviour,
        content_id: &ContentId,
    ) -> Result<(), Box<dyn std::error::Error>> {
        debug!("Getting access list for {} from DHT", content_id);
        let query_id = ContentRouter::get_access_list(behaviour, content_id);
        let mut queries = self.pending_queries.lock().await;
        queries.insert(query_id, PendingQuery::AccessListLookup {
            content_id: *content_id,
        });
        Ok(())
    }

    /// Handle libp2p swarm events related to DataCraft.
    pub async fn handle_swarm_event(&self, event: &SwarmEvent<CraftBehaviourEvent>) {
        match event {
            SwarmEvent::Behaviour(CraftBehaviourEvent::Kademlia(kad_event)) => {
                self.handle_kademlia_event(kad_event).await;
            }
            _ => {}
        }
    }

    /// Handle Kademlia DHT events.
    async fn handle_kademlia_event(&self, event: &libp2p::kad::Event) {
        use libp2p::kad::Event;

        match event {
            Event::OutboundQueryProgressed { id, result, .. } => {
                self.handle_query_result(*id, result).await;
            }
            _ => {}
        }
    }

    /// Handle the result of a DHT query.
    async fn handle_query_result(
        &self,
        query_id: kad::QueryId,
        result: &kad::QueryResult,
    ) {
        use libp2p::kad::{QueryResult, GetProvidersOk, GetRecordOk};

        let mut queries = self.pending_queries.lock().await;
        let pending = match queries.remove(&query_id) {
            Some(p) => p,
            None => {
                debug!("Received result for unknown query: {:?}", query_id);
                return;
            }
        };

        match (pending, result) {
            (PendingQuery::ProvidersLookup { content_id }, QueryResult::GetProviders(Ok(GetProvidersOk::FoundProviders { providers, .. }))) => {
                debug!("Found {} providers for {}", providers.len(), content_id);
                let provider_peers: Vec<libp2p::PeerId> = providers.iter().cloned().collect();

                let event = DataCraftEvent::ProvidersResolved {
                    content_id,
                    providers: provider_peers,
                };

                if let Err(e) = self.event_tx.send(event) {
                    error!("Failed to send providers resolved event: {}", e);
                }
            }

            (PendingQuery::ManifestLookup { content_id }, QueryResult::GetRecord(Ok(GetRecordOk::FoundRecord(peer_record)))) => {
                debug!("Found manifest record for {}", content_id);

                match datacraft_routing::parse_manifest_record(&peer_record.record.value) {
                    Ok(manifest) => {
                        let event = DataCraftEvent::ManifestRetrieved {
                            content_id,
                            manifest,
                        };

                        if let Err(e) = self.event_tx.send(event) {
                            error!("Failed to send manifest retrieved event: {}", e);
                        }
                    }
                    Err(e) => {
                        warn!("Failed to parse manifest record for {}: {}", content_id, e);
                        let event = DataCraftEvent::DhtError {
                            content_id,
                            error: format!("Failed to parse manifest: {}", e),
                        };

                        if let Err(e) = self.event_tx.send(event) {
                            error!("Failed to send DHT error event: {}", e);
                        }
                    }
                }
            }

            (PendingQuery::AccessListLookup { content_id }, QueryResult::GetRecord(Ok(GetRecordOk::FoundRecord(peer_record)))) => {
                debug!("Found access list record for {}", content_id);

                match datacraft_routing::parse_access_list_record(&peer_record.record.value) {
                    Ok(access_list) => {
                        let event = DataCraftEvent::AccessListRetrieved {
                            content_id,
                            access_list,
                        };
                        if let Err(e) = self.event_tx.send(event) {
                            error!("Failed to send access list retrieved event: {}", e);
                        }
                    }
                    Err(e) => {
                        warn!("Failed to parse access list record for {}: {}", content_id, e);
                        let event = DataCraftEvent::DhtError {
                            content_id,
                            error: format!("Failed to parse access list: {}", e),
                        };
                        if let Err(e) = self.event_tx.send(event) {
                            error!("Failed to send DHT error event: {}", e);
                        }
                    }
                }
            }

            (pending, result) => {
                debug!("Query {:?} completed with unhandled result: {:?}", pending, result);

                let content_id = match pending {
                    PendingQuery::ProvidersLookup { content_id } => content_id,
                    PendingQuery::ManifestLookup { content_id } => content_id,
                    PendingQuery::AccessListLookup { content_id } => content_id,
                };

                let error_msg = match result {
                    QueryResult::GetProviders(Err(e)) => format!("Providers lookup failed: {:?}", e),
                    QueryResult::GetRecord(Err(e)) => format!("Record lookup failed: {:?}", e),
                    _ => "Query completed with no results".to_string(),
                };

                let event = DataCraftEvent::DhtError {
                    content_id,
                    error: error_msg,
                };

                if let Err(e) = self.event_tx.send(event) {
                    error!("Failed to send DHT error event: {}", e);
                }
            }
        }
    }

    /// Request a piece from a remote peer with retry on transient errors.
    ///
    /// `piece_id` of all zeros means "any random piece for this segment".
    /// Returns (coefficients, data) on success.
    pub async fn request_piece_from_peer(
        &self,
        control: &mut libp2p_stream::Control,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
        segment_index: u32,
        piece_id: &[u8; 32],
    ) -> Result<(Vec<u8>, Vec<u8>), String> {
        let mut last_error = String::new();

        for attempt in 0..=MAX_RETRIES {
            if attempt > 0 {
                let backoff = Duration::from_secs(1 << (attempt - 1)); // 1s, 2s, 4s
                warn!(
                    "Retrying piece request {}/{} from {} (attempt {}/{}, backoff {:?})",
                    content_id, segment_index, peer_id, attempt, MAX_RETRIES, backoff
                );
                tokio::time::sleep(backoff).await;
            }

            match self.request_piece_from_peer_once(control, peer_id, content_id, segment_index, piece_id).await {
                Ok(result) => return Ok(result),
                Err(err) => {
                    if !err.should_retry() || attempt == MAX_RETRIES {
                        self.emit_transfer_error(content_id, &peer_id, &err);
                        return Err(format!("{} (peer {})", err, peer_id));
                    }
                    last_error = err.to_string();
                    debug!(
                        "Transient error requesting piece {}/{} from {}: {}",
                        content_id, segment_index, peer_id, last_error
                    );
                }
            }
        }

        Err(format!("Max retries exceeded for {}/{} from {}: {}", content_id, segment_index, peer_id, last_error))
    }

    /// Single attempt to request a piece from a peer.
    async fn request_piece_from_peer_once(
        &self,
        control: &mut libp2p_stream::Control,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
        segment_index: u32,
        piece_id: &[u8; 32],
    ) -> Result<(Vec<u8>, Vec<u8>), TransferError> {
        debug!(
            "Requesting piece {}/{} from peer {}",
            content_id, segment_index, peer_id
        );

        // Always open a fresh stream for piece requests to avoid
        // interference with push streams in the pool.
        let mut stream = tokio::time::timeout(
            Duration::from_secs(self.stream_open_timeout_secs),
            control.open_stream(peer_id, libp2p::StreamProtocol::new(TRANSFER_PROTOCOL)),
        )
        .await
        .map_err(|_| TransferError::Timeout)?
        .map_err(|e| {
            let msg = e.to_string();
            if msg.contains("refused") || msg.contains("Refused") {
                TransferError::PeerRefused
            } else {
                TransferError::ConnectionLost
            }
        })?;
        Self::do_request_piece(&mut stream, peer_id, content_id, segment_index, piece_id).await
    }

    /// Internal: perform piece request on an already-opened stream.
    async fn do_request_piece(
        stream: &mut libp2p::Stream,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
        segment_index: u32,
        piece_id: &[u8; 32],
    ) -> Result<(Vec<u8>, Vec<u8>), TransferError> {
        use futures::{AsyncReadExt, AsyncWriteExt};
        use datacraft_transfer::encode_piece_request;
        use datacraft_core::WireStatus;

        let request = encode_piece_request(content_id, segment_index, piece_id);
        write_and_flush(stream, &request).await?;

        let mut status_byte = [0u8; 1];
        timed_read(stream.read_exact(&mut status_byte)).await?;

        let status = WireStatus::from_u8(status_byte[0]).ok_or_else(|| {
            TransferError::InvalidData(format!("invalid status byte: {}", status_byte[0]))
        })?;

        let mut len_buf = [0u8; 4];
        timed_read(stream.read_exact(&mut len_buf)).await?;
        let coeff_len = u32::from_be_bytes(len_buf) as usize;

        match status {
            WireStatus::Ok => {
                // Validate coefficient length
                if coeff_len > MAX_COEFF_SIZE {
                    return Err(TransferError::InvalidData(
                        format!("coefficient length {} exceeds max {}", coeff_len, MAX_COEFF_SIZE),
                    ));
                }

                let mut coefficients = vec![0u8; coeff_len];
                timed_read(stream.read_exact(&mut coefficients)).await?;

                timed_read(stream.read_exact(&mut len_buf)).await?;
                let data_len = u32::from_be_bytes(len_buf) as usize;

                // Validate data length
                if data_len > MAX_PIECE_DATA_SIZE {
                    return Err(TransferError::InvalidData(
                        format!("data length {} exceeds max {}", data_len, MAX_PIECE_DATA_SIZE),
                    ));
                }

                let mut data = vec![0u8; data_len];
                timed_read(stream.read_exact(&mut data)).await?;

                debug!(
                    "Received piece {}/{} ({} bytes data, {} bytes coeff) from {}",
                    content_id, segment_index, data.len(), coefficients.len(), peer_id
                );

                Ok((coefficients, data))
            }
            WireStatus::NotFound => Err(TransferError::ContentNotFound),
            WireStatus::Error => Err(TransferError::PeerRefused),
        }
    }

    /// Push a manifest to a remote peer.
    pub async fn push_manifest_to_peer(
        &self,
        control: &mut libp2p_stream::Control,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
        manifest_json: &[u8],
    ) -> Result<(), String> {
        debug!("Pushing manifest for {} ({} bytes) to peer {}", content_id, manifest_json.len(), peer_id);

        let (stream_arc, use_pool) = if let Some(ref pool) = self.stream_pool {
            let mut p = pool.lock().await;
            if let Some(s) = p.get_stream(&peer_id) {
                (Some(s), true)
            } else {
                p.ensure_opening(peer_id);
                (None, true)
            }
        } else {
            (None, false)
        };

        let result = if let Some(stream_arc) = stream_arc {
            let mut stream = stream_arc.lock().await;
            Self::do_push_manifest(&mut *stream, peer_id, content_id, manifest_json).await
        } else {
            let mut stream = tokio::time::timeout(
                STREAM_OPEN_TIMEOUT,
                control.open_stream(peer_id, libp2p::StreamProtocol::new(TRANSFER_PROTOCOL)),
            )
            .await
            .map_err(|_| format!("Timed out opening stream to {}", peer_id))?
            .map_err(|e| format!("Failed to open stream to {}: {}", peer_id, e))?;
            Self::do_push_manifest(&mut stream, peer_id, content_id, manifest_json).await
        };

        if result.is_err() {
            if let Some(ref pool) = self.stream_pool {
                pool.lock().await.mark_dead(&peer_id);
            }
        }
        result
    }

    async fn do_push_manifest(
        stream: &mut libp2p::Stream,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
        manifest_json: &[u8],
    ) -> Result<(), String> {
        use futures::{AsyncReadExt, AsyncWriteExt};

        let msg = datacraft_transfer::encode_manifest_push(content_id, manifest_json);
        timed_write(stream.write_all(&msg)).await
            .map_err(|e| format!("Failed to send manifest push to {}: {}", peer_id, e))?;

        let mut status = [0u8; 1];
        timed_read(stream.read_exact(&mut status)).await
            .map_err(|e| format!("Failed to read manifest push response from {}: {}", peer_id, e))?;

        match datacraft_core::WireStatus::from_u8(status[0]) {
            Some(datacraft_core::WireStatus::Ok) => {
                debug!("Successfully pushed manifest for {} to {}", content_id, peer_id);
                Ok(())
            }
            _ => Err(format!("Manifest push rejected by peer {} with status {}", peer_id, status[0])),
        }
    }

    /// Push a piece to a remote peer for storage.
    pub async fn push_piece_to_peer(
        &self,
        control: &mut libp2p_stream::Control,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
        segment_index: u32,
        piece_id: &[u8; 32],
        coefficients: &[u8],
        piece_data: &[u8],
    ) -> Result<(), String> {
        debug!(
            "Pushing piece {}/{}/{} ({} bytes) to peer {}",
            content_id, segment_index, &hex::encode(&piece_id[..4]), piece_data.len(), peer_id
        );

        let (stream_arc, use_pool) = if let Some(ref pool) = self.stream_pool {
            let mut p = pool.lock().await;
            if let Some(s) = p.get_stream(&peer_id) {
                (Some(s), true)
            } else {
                p.ensure_opening(peer_id);
                (None, true)
            }
        } else {
            (None, false)
        };

        let result = if let Some(stream_arc) = stream_arc {
            let mut stream = stream_arc.lock().await;
            Self::do_push_piece(&mut *stream, peer_id, content_id, segment_index, piece_id, coefficients, piece_data).await
        } else {
            let mut stream = tokio::time::timeout(
                STREAM_OPEN_TIMEOUT,
                control.open_stream(peer_id, libp2p::StreamProtocol::new(TRANSFER_PROTOCOL)),
            )
            .await
            .map_err(|_| format!("Timed out opening stream to {}", peer_id))?
            .map_err(|e| format!("Failed to open stream to {}: {}", peer_id, e))?;
            Self::do_push_piece(&mut stream, peer_id, content_id, segment_index, piece_id, coefficients, piece_data).await
        };

        if result.is_err() {
            if let Some(ref pool) = self.stream_pool {
                pool.lock().await.mark_dead(&peer_id);
            }
        }
        result
    }

    async fn do_push_piece(
        stream: &mut libp2p::Stream,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
        segment_index: u32,
        piece_id: &[u8; 32],
        coefficients: &[u8],
        piece_data: &[u8],
    ) -> Result<(), String> {
        use futures::{AsyncReadExt, AsyncWriteExt};

        let msg = datacraft_transfer::encode_piece_push(content_id, segment_index, piece_id, coefficients, piece_data);
        timed_write(stream.write_all(&msg)).await
            .map_err(|e| format!("Failed to send push to {}: {}", peer_id, e))?;

        let mut status = [0u8; 1];
        timed_read(stream.read_exact(&mut status)).await
            .map_err(|e| format!("Failed to read push response from {}: {}", peer_id, e))?;

        match datacraft_core::WireStatus::from_u8(status[0]) {
            Some(datacraft_core::WireStatus::Ok) => {
                debug!("Successfully pushed piece {}/{} to {}", content_id, segment_index, peer_id);
                Ok(())
            }
            _ => Err(format!("Push rejected by peer {} with status {}", peer_id, status[0])),
        }
    }

    /// Handle an incoming transfer stream from a peer.
    ///
    /// Loops to handle multiple messages on the same persistent stream.
    /// Returns when the stream is closed (EOF) or an error occurs.
    pub async fn handle_incoming_stream(&self, mut stream: libp2p::Stream) {
        use futures::AsyncReadExt;

        debug!("Handling incoming transfer stream");

        loop {
            // Read enough bytes to determine message type.
            // All messages start with magic(4) + type(1).
            let mut type_header = [0u8; 5];
            match timed_read(stream.read_exact(&mut type_header)).await {
                Ok(_) => {}
                Err(TransferError::ConnectionLost) => {
                    debug!("Incoming stream closed (EOF/connection lost)");
                    return;
                }
                Err(TransferError::Timeout) => {
                    warn!("Incoming stream read timed out");
                    return;
                }
                Err(e) => {
                    error!("Failed to read type header: {}", e);
                    return;
                }
            }

            if type_header[0..4] != datacraft_core::WIRE_MAGIC {
                error!("Invalid magic bytes: {:02x?}", &type_header[0..4]);
                return;
            }

            let msg_type = type_header[4];

            if msg_type == datacraft_core::WireMessageType::ManifestPush as u8 {
                self.handle_incoming_manifest_push(&mut stream, &type_header).await;
                continue;
            }

            if msg_type == datacraft_core::WireMessageType::PiecePush as u8 {
                self.handle_incoming_piece_push(&mut stream, &type_header).await;
                continue;
            }

            if msg_type == datacraft_core::WireMessageType::PieceRequest as u8 {
                self.handle_incoming_piece_request(&mut stream, &type_header).await;
                continue;
            }

            if msg_type == datacraft_core::WireMessageType::InventoryRequest as u8 {
                self.handle_incoming_inventory_request(&mut stream, &type_header).await;
                continue;
            }

            error!("Unknown message type: {}", msg_type);
            return;
        }
    }

    /// Handle an incoming piece request.
    async fn handle_incoming_piece_request(&self, stream: &mut libp2p::Stream, type_header: &[u8; 5]) {
        use futures::{AsyncReadExt, AsyncWriteExt};

        // Read remaining bytes of piece request: content_id(32) + segment_index(4) + piece_id(32) = 68
        let mut rest = [0u8; 68];
        if let Err(e) = timed_read(stream.read_exact(&mut rest)).await {
            error!("Failed to read piece request body: {}", e);
            return;
        }

        // Reconstruct full header
        let mut full = [0u8; PIECE_REQUEST_SIZE];
        full[..5].copy_from_slice(type_header);
        full[5..].copy_from_slice(&rest);

        let (content_id, segment_index, piece_id) = match decode_piece_request(&full) {
            Ok(r) => r,
            Err(e) => {
                error!("Failed to decode piece request: {}", e);
                return;
            }
        };

        // Validate content_id is non-zero
        if content_id.0 == [0u8; 32] {
            warn!("Rejecting piece request with zero content_id");
            let response = encode_piece_response_error(datacraft_core::WireStatus::Error);
            let _ = write_and_flush(stream, &response).await;
            return;
        }

        // Pre-serve check: reject if content has been removed
        if let Some(ref cache) = self.removal_cache {
            let cache = cache.lock().await;
            if cache.is_removed(&content_id) {
                warn!("Refusing to serve removed content: {}", content_id);
                let response = encode_piece_response_error(datacraft_core::WireStatus::NotFound);
                let _ = write_and_flush(stream, &response).await;
                return;
            }
        }

        let store = self.store.lock().await;
        let is_any = piece_id == [0u8; 32];

        let result = if is_any {
            store.get_random_piece(&content_id, segment_index)
        } else {
            store
                .get_piece(&content_id, segment_index, &piece_id)
                .map(|(data, coeff)| Some((piece_id, data, coeff)))
        };

        match result {
            Ok(Some((_pid, data, coefficients))) => {
                debug!(
                    "Serving piece {}/{} ({} bytes data, {} bytes coeff)",
                    content_id, segment_index, data.len(), coefficients.len()
                );
                let response = encode_piece_response(datacraft_core::WireStatus::Ok, &coefficients, &data);
                if let Err(e) = write_and_flush(stream, &response).await {
                    error!("Failed to write response: {}", e);
                }
                // Record fetch for demand tracking (scaling)
                if let Some(ref dt) = self.demand_tracker {
                    let mut tracker = dt.lock().await;
                    tracker.record_fetch(content_id);
                }
                // Record access for eviction LRU tracking
                if let Some(ref em) = self.eviction_manager {
                    let mut mgr = em.lock().await;
                    mgr.record_access(&content_id);
                }
            }
            _ => {
                warn!("Piece not found: {}/{}", content_id, segment_index);
                let response = encode_piece_response_error(datacraft_core::WireStatus::NotFound);
                let _ = write_and_flush(stream, &response).await;
            }
        }
    }

    /// Handle an incoming piece push.
    async fn handle_incoming_piece_push(&self, stream: &mut libp2p::Stream, type_header: &[u8; 5]) {
        use futures::{AsyncReadExt, AsyncWriteExt};

        // PiecePush: [magic:4][type:1][content_id:32][segment_index:4][piece_id:32][coeff_len:4][coeff][data_len:4][data]
        // We already read 5 bytes. Read the rest of the fixed header: 32+4+32 = 68 more for fixed part
        let mut fixed_rest = [0u8; 68];
        if let Err(e) = timed_read(stream.read_exact(&mut fixed_rest)).await {
            error!("Failed to read piece push fixed header: {}", e);
            return;
        }

        // coeff_len(4)
        let mut coeff_len_buf = [0u8; 4];
        if let Err(e) = timed_read(stream.read_exact(&mut coeff_len_buf)).await {
            error!("Failed to read coeff_len: {}", e);
            return;
        }
        let coeff_len = u32::from_be_bytes(coeff_len_buf) as usize;

        // Validate coefficient length
        if coeff_len > MAX_COEFF_SIZE {
            warn!("Coefficient vector too large: {} bytes (max {})", coeff_len, MAX_COEFF_SIZE);
            let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            return;
        }

        let mut coefficients = vec![0u8; coeff_len];
        if coeff_len > 0 {
            if let Err(e) = timed_read(stream.read_exact(&mut coefficients)).await {
                error!("Failed to read coefficients: {}", e);
                return;
            }
        }

        // data_len(4)
        let mut data_len_buf = [0u8; 4];
        if let Err(e) = timed_read(stream.read_exact(&mut data_len_buf)).await {
            error!("Failed to read data_len: {}", e);
            return;
        }
        let data_len = u32::from_be_bytes(data_len_buf) as usize;

        if data_len > MAX_PIECE_DATA_SIZE {
            warn!("Push payload too large: {} bytes (max {})", data_len, MAX_PIECE_DATA_SIZE);
            let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            return;
        }

        let mut data = vec![0u8; data_len];
        if let Err(e) = timed_read(stream.read_exact(&mut data)).await {
            error!("Failed to read push data: {}", e);
            let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            return;
        }

        // Parse content_id, segment_index, piece_id from fixed_rest
        let mut cid_bytes = [0u8; 32];
        cid_bytes.copy_from_slice(&fixed_rest[0..32]);
        let content_id = ContentId(cid_bytes);
        let segment_index = u32::from_be_bytes([fixed_rest[32], fixed_rest[33], fixed_rest[34], fixed_rest[35]]);
        let mut piece_id = [0u8; 32];
        piece_id.copy_from_slice(&fixed_rest[36..68]);

        // Validate content_id is non-zero (32 bytes)
        if content_id.0 == [0u8; 32] {
            warn!("Rejecting piece push with zero content_id");
            let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            return;
        }

        // Validate piece_id matches SHA-256(coefficients)
        if coeff_len > 0 && !validate_piece_id(&piece_id, &coefficients) {
            warn!(
                "Piece ID mismatch for {}/{}: expected SHA-256(coefficients), got {}",
                content_id, segment_index, hex::encode(&piece_id[..8])
            );
            self.record_bad_data(&libp2p::PeerId::random()); // We don't have peer_id in this context
            let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            return;
        }

        // Check removal cache (fast, in-memory — do before ack)
        if let Some(ref cache) = self.removal_cache {
            let cache = cache.lock().await;
            if cache.is_removed(&content_id) {
                warn!("Refusing pushed piece for removed content: {}", content_id);
                let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
                return;
            }
        }

        // Check available disk space before accepting
        {
            let store_guard = self.store.lock().await;
            let store_path = store_guard.base_dir();
            if let Some(available) = available_disk_space(store_path) {
                if available < DISK_SPACE_THRESHOLD {
                    warn!(
                        "Refusing piece push: disk space low ({} bytes available, threshold {} bytes)",
                        available, DISK_SPACE_THRESHOLD
                    );
                    self.emit_storage_pressure(available);
                    let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
                    return;
                }
            }
        }

        // Ack immediately — data is in memory, storage happens async.
        // This unblocks the sender and frees the stream for the next piece.
        // Flush is critical: without it, the ack sits in yamux buffer and the sender hangs.
        let _ = write_and_flush(stream, &[datacraft_core::WireStatus::Ok as u8]).await;

        // Spawn storage work with bounded concurrency
        let store = self.store.clone();
        let merkle_tree = self.merkle_tree.clone();
        let event_tx = self.event_tx.clone();
        let sem = self.inbound_semaphore.clone();

        tokio::spawn(async move {
            let _permit = match sem.acquire().await {
                Ok(p) => p,
                Err(_) => return, // semaphore closed
            };

            let s = store.lock().await;
            match s.store_piece(&content_id, segment_index, &piece_id, &data, &coefficients) {
                Ok(()) => {
                    info!(
                        "Stored pushed piece {}/{}/{} ({} bytes)",
                        content_id, segment_index, &hex::encode(&piece_id[..4]), data.len()
                    );
                    drop(s); // release store lock before merkle
                    if let Some(ref mt) = merkle_tree {
                        mt.lock().await.insert(&content_id, segment_index, &piece_id);
                    }
                    let _ = event_tx.send(DataCraftEvent::PiecePushReceived { content_id });
                }
                Err(e) => {
                    error!("Failed to store pushed piece: {}", e);
                }
            }
        });
    }

    /// Handle an incoming manifest push.
    async fn handle_incoming_manifest_push(&self, stream: &mut libp2p::Stream, type_header: &[u8; 5]) {
        use futures::{AsyncReadExt, AsyncWriteExt};

        // ManifestPush header: [magic:4][type:1][content_id:32][payload_len:4] = 41 bytes
        // We read 5. Need 36 more.
        let mut rest = [0u8; 36];
        if let Err(e) = timed_read(stream.read_exact(&mut rest)).await {
            error!("Failed to read manifest push header: {}", e);
            return;
        }

        let mut full_header = [0u8; MANIFEST_PUSH_HEADER_SIZE];
        full_header[..5].copy_from_slice(type_header);
        full_header[5..].copy_from_slice(&rest);

        let (content_id, payload_len) = match decode_manifest_push_header(&full_header) {
            Ok(v) => v,
            Err(e) => {
                error!("Failed to decode manifest push header: {}", e);
                let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
                return;
            }
        };

        // Validate content_id
        if content_id.0 == [0u8; 32] {
            warn!("Rejecting manifest push with zero content_id");
            let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            return;
        }

        if payload_len as usize > MAX_MANIFEST_SIZE {
            warn!("Manifest push payload too large: {} bytes (max {})", payload_len, MAX_MANIFEST_SIZE);
            let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            return;
        }

        let mut payload = vec![0u8; payload_len as usize];
        if let Err(e) = timed_read(stream.read_exact(&mut payload)).await {
            error!("Failed to read manifest push payload: {}", e);
            let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            return;
        }

        match serde_json::from_slice::<ContentManifest>(&payload) {
            Ok(manifest) => {
                // Validate manifest content_id matches the header
                if manifest.content_id != content_id {
                    warn!(
                        "Manifest content_id mismatch: header={}, body={}",
                        content_id, manifest.content_id
                    );
                    let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
                    return;
                }

                let store = self.store.lock().await;
                match store.store_manifest(&manifest) {
                    Ok(()) => {
                        info!("Stored pushed manifest for {} ({} bytes)", content_id, payload_len);
                        let _ = self.event_tx.send(DataCraftEvent::ManifestPushReceived {
                            content_id,
                            manifest,
                        });
                        let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Ok as u8])).await;
                    }
                    Err(e) => {
                        error!("Failed to store pushed manifest: {}", e);
                        let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
                    }
                }
            }
            Err(e) => {
                error!("Failed to parse pushed manifest for {}: {}", content_id, e);
                let _ = timed_write(stream.write_all(&[datacraft_core::WireStatus::Error as u8])).await;
            }
        }
    }

    /// Handle an incoming inventory request.
    async fn handle_incoming_inventory_request(&self, stream: &mut libp2p::Stream, type_header: &[u8; 5]) {
        use futures::{AsyncReadExt, AsyncWriteExt};

        // InventoryRequest: [magic:4][type:1][content_id:32] — we already read 5, need 32 more
        let mut cid_bytes = [0u8; 32];
        if let Err(e) = timed_read(stream.read_exact(&mut cid_bytes)).await {
            error!("Failed to read inventory request body: {}", e);
            return;
        }
        let content_id = ContentId(cid_bytes);

        // Validate content_id
        if content_id.0 == [0u8; 32] {
            warn!("Rejecting inventory request with zero content_id");
            let response = datacraft_transfer::encode_inventory_response_error(datacraft_core::WireStatus::Error);
            let _ = timed_write(stream.write_all(&response)).await;
            return;
        }

        // Check removal cache
        if let Some(ref cache) = self.removal_cache {
            let cache = cache.lock().await;
            if cache.is_removed(&content_id) {
                let response = datacraft_transfer::encode_inventory_response_error(datacraft_core::WireStatus::NotFound);
                let _ = timed_write(stream.write_all(&response)).await;
                return;
            }
        }

        let store = self.store.lock().await;
        let manifest = match store.get_manifest(&content_id) {
            Ok(m) => m,
            Err(_) => {
                let response = datacraft_transfer::encode_inventory_response_error(datacraft_core::WireStatus::NotFound);
                let _ = timed_write(stream.write_all(&response)).await;
                return;
            }
        };

        let mut segments = Vec::new();
        for seg_idx in 0..manifest.segment_count as u32 {
            let piece_ids = match store.list_pieces(&content_id, seg_idx) {
                Ok(ids) => ids,
                Err(_) => continue,
            };
            let mut coefficient_vectors = Vec::new();
            for pid in &piece_ids {
                if let Ok((_data, coeff)) = store.get_piece(&content_id, seg_idx, pid) {
                    coefficient_vectors.push(coeff);
                }
            }
            if !coefficient_vectors.is_empty() {
                segments.push(datacraft_core::SegmentInventory {
                    segment_index: seg_idx,
                    coefficient_vectors,
                });
            }
        }

        let response = datacraft_core::InventoryResponse {
            content_id,
            segments,
        };
        let encoded = datacraft_transfer::encode_inventory_response(&response);
        if let Err(e) = timed_write(stream.write_all(&encoded)).await {
            error!("Failed to write inventory response: {}", e);
        }
    }

    /// Request inventory from a remote peer.
    pub async fn request_inventory_from_peer(
        &self,
        control: &mut libp2p_stream::Control,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
    ) -> Result<datacraft_core::InventoryResponse, String> {
        debug!("Requesting inventory for {} from peer {}", content_id, peer_id);

        let (stream_arc, use_pool) = if let Some(ref pool) = self.stream_pool {
            let mut p = pool.lock().await;
            if let Some(s) = p.get_stream(&peer_id) {
                (Some(s), true)
            } else {
                p.ensure_opening(peer_id);
                (None, true)
            }
        } else {
            (None, false)
        };

        let result = if let Some(stream_arc) = stream_arc {
            let mut stream = stream_arc.lock().await;
            Self::do_request_inventory(&mut *stream, peer_id, content_id).await
        } else {
            let mut stream = tokio::time::timeout(
                STREAM_OPEN_TIMEOUT,
                control.open_stream(peer_id, libp2p::StreamProtocol::new(datacraft_core::TRANSFER_PROTOCOL)),
            )
            .await
            .map_err(|_| format!("Timed out opening stream to {}", peer_id))?
            .map_err(|e| format!("Failed to open stream to {}: {}", peer_id, e))?;
            Self::do_request_inventory(&mut stream, peer_id, content_id).await
        };

        if result.is_err() {
            if let Some(ref pool) = self.stream_pool {
                pool.lock().await.mark_dead(&peer_id);
            }
        }
        result
    }

    async fn do_request_inventory(
        stream: &mut libp2p::Stream,
        peer_id: libp2p::PeerId,
        content_id: &ContentId,
    ) -> Result<datacraft_core::InventoryResponse, String> {
        use futures::{AsyncReadExt, AsyncWriteExt};

        let request = datacraft_transfer::encode_inventory_request(content_id);
        timed_write(stream.write_all(&request)).await
            .map_err(|e| format!("Failed to send inventory request to {}: {}", peer_id, e))?;

        let mut status_byte = [0u8; 1];
        timed_read(stream.read_exact(&mut status_byte)).await
            .map_err(|e| format!("Failed to read inventory status from {}: {}", peer_id, e))?;

        let status = datacraft_core::WireStatus::from_u8(status_byte[0]).ok_or_else(|| {
            format!("Invalid inventory status byte from {}: {}", peer_id, status_byte[0])
        })?;

        let mut len_buf = [0u8; 4];
        timed_read(stream.read_exact(&mut len_buf)).await
            .map_err(|e| format!("Failed to read inventory payload_len from {}: {}", peer_id, e))?;
        let payload_len = u32::from_be_bytes(len_buf) as usize;

        // Validate payload length
        if payload_len > MAX_INVENTORY_SIZE {
            return Err(format!("Inventory payload too large from {}: {} bytes (max {})", peer_id, payload_len, MAX_INVENTORY_SIZE));
        }

        match status {
            datacraft_core::WireStatus::Ok => {
                let mut payload = vec![0u8; payload_len];
                timed_read(stream.read_exact(&mut payload)).await
                    .map_err(|e| format!("Failed to read inventory payload from {}: {}", peer_id, e))?;
                let response: datacraft_core::InventoryResponse = bincode::deserialize(&payload)
                    .map_err(|e| format!("Failed to deserialize inventory from {}: {}", peer_id, e))?;
                Ok(response)
            }
            datacraft_core::WireStatus::NotFound => {
                Err(format!("Inventory not found on peer {}", peer_id))
            }
            datacraft_core::WireStatus::Error => {
                Err(format!("Inventory error from peer {}", peer_id))
            }
        }
    }
}
